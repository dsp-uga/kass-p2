{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","from pyspark.sql import *\n","from pyspark.sql.functions import col\n","\n","from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes, OneVsRest\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import LongType"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import cv2   \n","import numpy as np\n","import urllib.request\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import multiprocessing\n","from joblib import Parallel, delayed\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["spark = SparkSession.builder.appName('deneme1').getOrCreate()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["input_x_small_train_file = \"gs://uga-dsp/project2/files/X_small_train.csv\"\n","input_x_small_test_file = \"gs://uga-dsp/project2/files/X_small_test.csv\"\n","\n","df_small_train = spark.read.option(\"header\", \"true\").csv(input_x_small_train_file)\n","df_small_test = spark.read.option(\"header\", \"true\").csv(input_x_small_test_file)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["input_xtrain_file = \"gs://uga-dsp/project2/files/X_train.csv\"\n","\n","input_xatest_file = \"gs://uga-dsp/project2/files/Xa_test.csv\"\n","input_xbtest_file = \"gs://uga-dsp/project2/files/Xb_test.csv\"\n","input_xctest_file = \"gs://uga-dsp/project2/files/Xc_test.csv\"\n","\n","df_training = spark.read.option(\"header\", \"true\").csv(input_xtrain_file)\n","\n","dfa_test = spark.read.option(\"header\", \"true\").csv(input_xatest_file)\n","dfb_test = spark.read.option(\"header\", \"true\").csv(input_xbtest_file)\n","dfc_test = spark.read.option(\"header\", \"true\").csv(input_xctest_file)\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def save_faces(image, folder_name=False):\n","    path = f'https://storage.googleapis.com/uga-dsp/project2/images/{image[0]}'\n","    x1 = int(image[1])\n","    y1 = int(image[2])\n","    x2 = int(image[3])\n","    y2 = int(image[4])\n","    label = image[5]\n","    face_id = image[6]\n","    \n","    if not folder_name:\n","        folder_name = label\n","    try:\n","        with urllib.request.urlopen(path) as resp:\n","            downloaded_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n","            downloaded_image = cv2.imdecode(downloaded_image, cv2.IMREAD_COLOR)\n","            cropped = downloaded_image[y1:y2, x1:x2]\n","            resized =  cv2.resize(cropped, (80,80))\n","            local_path = f'/home/ubuntu/data/faces_small/test/{folder_name}/{face_id}.jpg'\n","            cv2.imwrite(local_path, resized)\n","    except:\n","        print(f'An exception occurred with {image[0]}, face - {face_id}')\n","    return path"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def process_images(X_dataframe):\n","    last_column = len(X_dataframe.columns) - 1\n","    images = X_dataframe.select([X_dataframe.columns[2],X_dataframe.columns[5],X_dataframe.columns[6],X_dataframe.columns[7],X_dataframe.columns[8], X_dataframe.columns[last_column], X_dataframe.columns[1]])\n","    images_list = images.collect()\n","    return images_list"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def download_images(images, folder_name=False):\n","    num_cores = multiprocessing.cpu_count()\n","    inputs = tqdm(images)\n","\n","    if __name__ == \"__main__\":\n","        processed_list = Parallel(n_jobs=num_cores)(delayed(save_faces)(i, folder_name) for i in inputs)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# images = process_images(df_training)\n","# download_images(images)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# test_a_images = process_images(dfa_test)\n","# download_images(test_a_images, \"test_a\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# test_b_images = process_images(dfb_test)\n","# download_images(test_b_images, \"test_b\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 16612/16612 [08:25<00:00, 32.89it/s]\n"]}],"source":["# test_c_images = process_images(dfc_test)\n","# download_images(test_c_images, \"test_c\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 263/263 [00:07<00:00, 34.80it/s]\n"]}],"source":["small_test_images = process_images(df_small_test)\n","download_images(small_test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}